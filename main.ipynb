{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import copy\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    \"\"\"\n",
    "    Vertex class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"\n",
    "        Constructor for the Vertex class\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.parents = (\n",
    "            set()\n",
    "        )  # set consisting of Vertex objects that are parents of this vertex\n",
    "        self.children = (\n",
    "            set()\n",
    "        )  # set consisting of Vertex objects that are children of this vertex\n",
    "\n",
    "\n",
    "class CausalDAG:\n",
    "    \"\"\"\n",
    "    DAG class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vertex_names: list[str], edges: list[(str, str)]) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the causal DAG class\n",
    "        \"\"\"\n",
    "\n",
    "        self.vertices = {\n",
    "            v: Vertex(v) for v in vertex_names\n",
    "        }  # dictionary mapping vertex names to Vertex objects\n",
    "        self.edges = []  # list of tuples corresponding to edges in the DAG\n",
    "\n",
    "        # loop over and initialize all vertices to have parent-child relations specified by the edges\n",
    "        for parent_name, child_name in edges:\n",
    "            self.edges.append((parent_name, child_name))\n",
    "            # get the corresponding vertex objects\n",
    "            parent_vertex = self.vertices.get(parent_name)\n",
    "            child_vertex = self.vertices.get(child_name)\n",
    "            # add to the parent/child sets\n",
    "            parent_vertex.children.add(child_vertex)\n",
    "            child_vertex.parents.add(parent_vertex)\n",
    "\n",
    "    def get_parents(self, vertex_name: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of names of the parents\n",
    "        \"\"\"\n",
    "        return [p.name for p in self.vertices[vertex_name].parents]\n",
    "\n",
    "    def get_children(self, vertex_name: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of names of the parents\n",
    "        \"\"\"\n",
    "        return [c.name for c in self.vertices[vertex_name].children]\n",
    "\n",
    "    def get_descendants(self, vertex_name: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns a list of strings corresponding to descendants of the given vertex.\n",
    "        Note by convention, the descendants of a vertex include the vertex itself.\n",
    "        \"\"\"\n",
    "\n",
    "        stack = [vertex_name]\n",
    "        visited = set()\n",
    "\n",
    "        while len(stack) > 0:\n",
    "\n",
    "            v_name = stack.pop()\n",
    "            if v_name in visited:\n",
    "                continue\n",
    "            visited.add(v_name)\n",
    "            stack += self.get_children(v_name)\n",
    "\n",
    "        return list(visited)\n",
    "\n",
    "    def d_separated(self, x_name: str, y_name: str, z_names: list[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if X _||_ Y | Z using d-separation\n",
    "        \"\"\"\n",
    "\n",
    "        stack = [(x_name, \"up\")]  # stack for vertices to be explored next\n",
    "        visited = (\n",
    "            set()\n",
    "        )  # set of (vertex, direction) pairs that have already been explored\n",
    "\n",
    "        while len(stack) > 0:\n",
    "\n",
    "            v_name, direction = stack.pop()\n",
    "\n",
    "            if (v_name, direction) in visited:\n",
    "                continue\n",
    "\n",
    "            # we reached Y through an open path so return False\n",
    "            if v_name == y_name:\n",
    "                return False\n",
    "\n",
    "            visited.add((v_name, direction))\n",
    "\n",
    "            # cases for active forks and chain\n",
    "            if direction == \"up\" and v_name not in z_names:\n",
    "\n",
    "                for child in self.get_children(v_name):\n",
    "                    stack.append((child, \"down\"))\n",
    "                for parent in self.get_parents(v_name):\n",
    "                    stack.append((parent, \"up\"))\n",
    "\n",
    "            # cases for active chain and colliders\n",
    "            elif direction == \"down\":\n",
    "\n",
    "                if v_name not in z_names:\n",
    "                    for child in self.get_children(v_name):\n",
    "                        stack.append((child, \"down\"))\n",
    "\n",
    "                if len(set(self.get_descendants(v_name)).intersection(z_names)) != 0:\n",
    "                    for parent in self.get_parents(v_name):\n",
    "                        stack.append((parent, \"up\"))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def valid_backdoor_set(self, a_name: str, y_name: str, z_names: list[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if Z is a valid backdoor set for computing the effect of A on Y\n",
    "        \"\"\"\n",
    "\n",
    "        # check the descendants criterion\n",
    "        descendants_a = self.get_descendants(a_name)\n",
    "        if len(set(descendants_a).intersection(z_names)) != 0:\n",
    "            return False\n",
    "\n",
    "        # check d-sep criterion in graph where we remove outgoing edges A->o\n",
    "        edges = []\n",
    "        for edge in self.edges:\n",
    "            if edge[0] != a_name:\n",
    "                edges.append(edge)\n",
    "\n",
    "        G_Abar = CausalDAG(self.vertices, edges)\n",
    "        return G_Abar.d_separated(a_name, y_name, z_names)\n",
    "\n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Method for visualizing the DAG\n",
    "        \"\"\"\n",
    "\n",
    "        dot = Digraph()\n",
    "        dot.graph_attr[\"rankdir\"] = \"LR\"\n",
    "\n",
    "        for v_name in self.vertices:\n",
    "            dot.node(\n",
    "                v_name,\n",
    "                shape=\"plaintext\",\n",
    "                height=\".5\",\n",
    "                width=\".5\",\n",
    "            )\n",
    "\n",
    "        for parent, child in self.edges:\n",
    "            dot.edge(parent, child, color=\"blue\")\n",
    "\n",
    "        return dot\n",
    "\n",
    "\n",
    "def fit_backdoor_model(\n",
    "    data: pd.DataFrame, a_name: str, y_name: str, z_names: list[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    ANGUS: helper function for code reuse\n",
    "    \"\"\"\n",
    "    # make a regression formula\n",
    "    z_names = [\"1\"] + z_names\n",
    "    z_formula = \" + \".join(z_names)\n",
    "    regression_formula = f\"{y_name} ~ {z_formula} + {a_name}\"\n",
    "\n",
    "    # fit a regression depending on whether Y is binary or not\n",
    "    if set(data[y_name]) == {0, 1}:\n",
    "        model = smf.glm(\n",
    "            formula=regression_formula, family=sm.families.Binomial(), data=data\n",
    "        ).fit()\n",
    "    else:\n",
    "        model = smf.glm(\n",
    "            formula=regression_formula, family=sm.families.Gaussian(), data=data\n",
    "        ).fit()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def backdoor_adjustment(\n",
    "    data: pd.DataFrame, a_name: str, y_name: str, z_names: list[str]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform backdoor adjustment for a given treatment A and outcome Y using\n",
    "    the covariates in Z\n",
    "    \"\"\"\n",
    "    model = fit_backdoor_model(data, a_name, y_name, z_names)\n",
    "\n",
    "    data_a1 = data.copy()  # make a copy for the interventional datasets\n",
    "    data_a1[a_name] = 1\n",
    "    data_a0 = data.copy()\n",
    "    data_a0[a_name] = 0\n",
    "\n",
    "    return round(np.mean(model.predict(data_a1) - model.predict(data_a0)), 3)\n",
    "\n",
    "\n",
    "def fit_ipw_model(data: pd.DataFrame, a_name: str, y_name: str, z_names: list[str]):\n",
    "    \"\"\"\n",
    "    ANGUS: helper function for code reuse\n",
    "    \"\"\"\n",
    "    # fit a binomial regression to find p(A | Z)\n",
    "    z_names = [\"1\"] + z_names\n",
    "    z_formula = \" + \".join(z_names)\n",
    "    regression_formula = f\"{a_name} ~ {z_formula}\"\n",
    "    model = smf.glm(\n",
    "        formula=regression_formula, family=sm.families.Binomial(), data=data\n",
    "    ).fit()\n",
    "    return model\n",
    "\n",
    "\n",
    "def ipw(data: pd.DataFrame, a_name: str, y_name: str, z_names: list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Perform IPW for a given treatment A and outcome Y using\n",
    "    the covariates in Z\n",
    "    \"\"\"\n",
    "    model = fit_ipw_model(data, a_name, y_name, z_names)\n",
    "\n",
    "    p_a1_z = model.predict(data)\n",
    "    p_a0_z = 1 - p_a1_z\n",
    "\n",
    "    # I(A_i = 1) is the same as data[a_name]\n",
    "    e_ya1 = np.mean((data[a_name] / p_a1_z) * data[y_name])\n",
    "    e_ya0 = np.mean(((1 - data[a_name]) / p_a0_z) * data[y_name])\n",
    "\n",
    "    return round(e_ya1 - e_ya0, 3)\n",
    "\n",
    "\n",
    "def augmented_ipw(\n",
    "    data: pd.DataFrame, a_name: str, y_name: str, z_names: list[str]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform AIPW for a given treatment A and outcome Y using\n",
    "    the covariates in Z\n",
    "    \"\"\"\n",
    "    backdoor_model = fit_backdoor_model(data, a_name, y_name, z_names)\n",
    "    ipw_model = fit_ipw_model(data, a_name, y_name, z_names)\n",
    "\n",
    "    e_y_given_ac = backdoor_model.predict(data)\n",
    "\n",
    "    p_a1_z = ipw_model.predict(data)\n",
    "    p_a0_z = 1 - p_a1_z\n",
    "\n",
    "    data_a1 = data.copy()  # make a copy for the interventional datasets\n",
    "    data_a1[a_name] = 1\n",
    "    data_a0 = data.copy()\n",
    "    data_a0[a_name] = 0\n",
    "\n",
    "    # E[Y^a1] = E[(I[A=1]/P(A=1|Z)) * (Y-E[Y|A,Z]) + E[Y|A=1, Z]]\n",
    "    e_ya1 = np.mean(\n",
    "        (data[a_name] / p_a1_z) * (data[y_name] - e_y_given_ac)\n",
    "        + backdoor_model.predict(data_a1)\n",
    "    )\n",
    "    # E[Y^a0] = E[(I[A=0]/P(A=0|Z)) * (Y-E[Y|A,Z]) + E[Y|A=0, Z]]\n",
    "    e_ya0 = np.mean(\n",
    "        ((1 - data[a_name]) / p_a0_z) * (data[y_name] - e_y_given_ac)\n",
    "        + backdoor_model.predict(data_a0)\n",
    "    )\n",
    "\n",
    "    return round(e_ya1 - e_ya0, 3)\n",
    "\n",
    "\n",
    "def compute_confidence_intervals(\n",
    "    data: pd.DataFrame,\n",
    "    a_name: str,\n",
    "    y_name: str,\n",
    "    z_names: list[str],\n",
    "    estimator: Callable,\n",
    "    num_bootstraps: int = 200,\n",
    "    alpha: float = 0.05,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute confidence intervals for a given estimator via bootstrap\n",
    "\n",
    "    Returns tuple (q_low, q_up) for the lower and upper quantiles of the confidence interval.\n",
    "    \"\"\"\n",
    "\n",
    "    Ql = alpha / 2\n",
    "    Qu = 1 - alpha / 2\n",
    "    estimates = []\n",
    "\n",
    "    for i in range(num_bootstraps):\n",
    "\n",
    "        # resample the data with replacement\n",
    "        data_sampled = data.sample(len(data), replace=True)\n",
    "        data_sampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # add estimate from resampled data\n",
    "        estimates.append(estimator(data_sampled, a_name, y_name, z_names))\n",
    "\n",
    "    # calculate the quantiles\n",
    "    quantiles = np.quantile(estimates, q=[Ql, Qu])\n",
    "    q_low = quantiles[0]\n",
    "    q_up = quantiles[1]\n",
    "    return round(q_low, 3), round(q_up, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = set()\n",
    "edges = []\n",
    "\n",
    "# Quick parser to get vertices and edges\n",
    "with open(\"pytetrad/PROJECT_learned_edges.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[4:91]:\n",
    "        split = line.split()\n",
    "        vertices.add(split[1])\n",
    "        vertices.add(split[3])\n",
    "        edges.append((split[1], split[3]))\n",
    "\n",
    "dag = CausalDAG(list(vertices), edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dag.draw() # DAG.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
